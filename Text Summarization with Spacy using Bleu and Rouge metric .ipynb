{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adjacent-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "stopwords = list (STOP_WORDS)\n",
    "punctuation += '\\n'\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alternative-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a Class for extractive Summarization \n",
    "\n",
    "class Summarizer():\n",
    "    \n",
    "    ## constructor for data loading\n",
    "    ## give either path or corpus itself\n",
    "    def __init__(self, path = None, corpus = None):\n",
    "        self.mCorpus = ''\n",
    "        self.mWordFrequencies = {}\n",
    "        self.mSentScore = {}\n",
    "        self.mNumSentences = 0\n",
    "        \n",
    "        if path != None:\n",
    "            file = open(path, \"r\")\n",
    "            self.mCorpus = self.mCorpus + file.read()\n",
    "        elif corpus != None:\n",
    "            self.mCorpus = corpus\n",
    "            \n",
    "        self.mNlp = spacy.load('en_core_web_sm')\n",
    "        self.mDoc = self.mNlp(self.mCorpus)\n",
    "    \n",
    "    \n",
    "    # Function for Printing Corpus\n",
    "    def PrintCorpus(self):\n",
    "        print(self.mCorpus)\n",
    "    \n",
    "    \n",
    "    ## defining a function for Calculating \n",
    "    ## word frequiences\n",
    "    def WordFrequencyCalculator(self):\n",
    "        for word in self.mDoc:\n",
    "            wordInLowerCase = word.text.lower()\n",
    "            \n",
    "            if (wordInLowerCase not in stopwords) and (wordInLowerCase not in punctuation):\n",
    "                if wordInLowerCase not in self.mWordFrequencies.keys():\n",
    "                    self.mWordFrequencies[wordInLowerCase] = 1\n",
    "                else:\n",
    "                    self.mWordFrequencies[wordInLowerCase] += 1\n",
    "    \n",
    "        return\n",
    "    \n",
    "    \n",
    "    ## Function for normalizing Frequency Values\n",
    "    def WordFreqNormalizer(self):\n",
    "        maxFreq = max(self.mWordFrequencies.values())\n",
    "        \n",
    "        for word in self.mWordFrequencies.keys():\n",
    "            self.mWordFrequencies[word] = self.mWordFrequencies[word] / maxFreq\n",
    "        \n",
    "        return\n",
    "    \n",
    "    # Function For calculating  sentence score\n",
    "    # based on computed normalized word Frequencies\n",
    "    def CalSentScore(self):\n",
    "        sentences = [sent for sent in self.mDoc.sents]\n",
    "        self.mNumSentences = len(sentences)\n",
    "        \n",
    "        for sent in sentences:\n",
    "            for word in sent:\n",
    "                wordInLowerCase = word.text.lower()\n",
    "                \n",
    "                if wordInLowerCase in self.mWordFrequencies.keys():\n",
    "                    if sent not in self.mSentScore.keys():\n",
    "                        self.mSentScore[sent] = self.mWordFrequencies[wordInLowerCase]\n",
    "                    else:\n",
    "                        self.mSentScore[sent] += self.mWordFrequencies[wordInLowerCase]\n",
    "        return\n",
    "    \n",
    "    ## Writting the MainFunction for this script now\n",
    "    def SummarizeMyText(self, fractionToReduce = 0.2):\n",
    "        self.WordFrequencyCalculator()\n",
    "        self.WordFreqNormalizer()\n",
    "        self.CalSentScore()\n",
    "        \n",
    "        reducedSentNum = int (self.mNumSentences * fractionToReduce)\n",
    "        print ('Total number of Sentences = {}'.format(self.mNumSentences))\n",
    "        print('Num of Sentences Reduced to {}'.format(reducedSentNum))\n",
    "        print('Summary as follows : \\n')\n",
    "        \n",
    "        summaryList = nlargest(reducedSentNum, self.mSentScore, key = self.mSentScore.get)\n",
    "        for sent in summaryList:\n",
    "            print(sent, end = '')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "electric-julian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Summary for content.txt \n",
      "\n",
      "Total number of Sentences = 65\n",
      "Num of Sentences Reduced to 3\n",
      "Summary as follows : \n",
      "\n",
      "Personally I feel like statistics and probability are subjects that hold strong intuitions behind that are only perceived by facing examples and exercises, however the content was mainly explained from a rather aseptic theoretical point of view.I found the last parts of the class really rushed and i had to work a lot after class just to understand what we saw earlier so i had no time to do exercices.He was very explicit and always motivate the class ahead about every topic to be addressed\n",
      "\"It was over complicated, the explanation was not clear to me, it was over complicated thought when I went to study alone from external resources it was easier for me.\n",
      "\n",
      "Printing Summary for diffucult.txt \n",
      "\n",
      "Total number of Sentences = 53\n",
      "Num of Sentences Reduced to 2\n",
      "Summary as follows : \n",
      "\n",
      "The class content is easy but may have hard applications if the professor wants to\n",
      "The content was easy, the applications with R were a little bit difficult because the professor didn't explain the R functions syntax and how they work.\n",
      "\n",
      "Printing Summary for expectation_fsml.txt \n",
      "\n",
      "Total number of Sentences = 61\n",
      "Num of Sentences Reduced to 3\n",
      "Summary as follows : \n",
      "\n",
      "It was a statistics discovery, I would have appreciated to learn some basics, I had the feeling we went through tuff theroems straight away\n",
      "the class had all the probabilityI was expecting a point of view more conceptually visual (with this I mean based on real examples rather than a series of slides displaying the theoretical part black over white with coins and dices examples from time to time).The class did cover everything about probability\n",
      "Class was good but need some exercise sheet so that we can practice\n",
      "\n",
      "\n",
      "Printing Summary for least_fsml1.txt \n",
      "\n",
      "Total number of Sentences = 108\n",
      "Num of Sentences Reduced to 5\n",
      "Summary as follows : \n",
      "\n",
      "Like Pr Auroux, it would be better to explain first, take examples on board - have a Q&A and then move to explain it in R\n",
      "- Examples: It would be extremely helpful to take fewer examples and explain step by step on blackboard and then transit to R at a slower pace as many of us don't have an R background\n",
      "- Blackboard: An stylus based tablet/laptop will easier for the professor to write and us to understand what is written, so the blackboard notes would be helpful for a long-time\n",
      "- Time split between theory & R: As I understand it's the foundation course on Statistics and its final assessment will be taken as MCQ like previous course, it would be better to give theory and solving problems on blackboard/slides more proportion of the class than practising more examples on R\"\n",
      "Some of the examples.. such like \"The probability you have HIV\", \"The probability you have breast cancer\" etc... I started to feel dizzy at a moment after combo of these kind of examples... \n",
      "\"- The distraction with theory, slides, examples, blackboard, R\n",
      "- Should be moving gently between all of these; first, theory then step by step \n",
      "  examples on black board, thenThe use of R Studio while explaining certain concepts may have been a bit too much, i would have liked if we had a morning or an afternoon dedicated to the use of R in descriptive statistics and probabilites.The first day were R was used intensely was very difficult to follow, because I didn't had prior training in R and learning R was not included on the bootcamp. \n",
      "\"Quite fast-paced: the teacher obviously knows his topic and explains it very fast, sometime a little bit too fast...\n",
      "\n",
      "\n",
      "Printing Summary for main_fsml.txt \n",
      "\n",
      "Total number of Sentences = 78\n",
      "Num of Sentences Reduced to 3\n",
      "Summary as follows : \n",
      "\n",
      "I learned probability theory, random variables etc\n",
      "Multiple Random Variables, Convergence, Probability theory, Stochastic processes, Illustrations in R\n",
      "Better understanding of probability theory.I learned basic statistics terms and graphs, mainly we were dealing these days with probability theory: expected value, sample set, events, random values, stochastic processes etc\n",
      "Basics of descriptive statistics, and then probability theory with random variable and multiple variable courses \n",
      "Probabiloty in english\n",
      "Probability, Statistics, Random Variable theory, convergence....\n",
      "already replied in a previous survey\n",
      "Descriptive analysis, distributive probabilites, random variables and random vectors...\n",
      "\n",
      "Printing Summary for most_fsml.txt \n",
      "\n",
      "Total number of Sentences = 104\n",
      "Num of Sentences Reduced to 5\n",
      "Summary as follows : \n",
      "\n",
      "The professor's willingness to provide further explanations and examples to make sure that all students understand the course.The architecture of the course was good, that was concise, the slides were clearly written, and the examples were pertinent\n",
      "The overall flow of knowledge brought throughout the course, but mainly his use of R studio and the way to apply what we learned in R studio\n",
      "Topics he choose\n",
      "progression, nice examples of real life cases , some times counter intuitives.The practical examples and exercices, the pdf, a well structured course, with recap.\n",
      "\n",
      "Printing Summary for not_cover.txt \n",
      "\n",
      "Total number of Sentences = 86\n",
      "Num of Sentences Reduced to 4\n",
      "Summary as follows : \n",
      "\n",
      "Basics of R without jumping on scripts, functions and complex problems \n",
      "a dedicated period of time about how to use R in statistics\n",
      "\n",
      "under the DA's point of view, I would like to have more lessons in the statistic than only probability ex: distribution, test, PCA... because it's an important part of Data Analysis to understand and interpret the trend of data and it will be taught only in the FSML part 2.Revision of basic probability and statistics before starting with the more complex topics: maybe could be done during the Bootcamp (during the Bootcamp we only did a little bit of combinatorics, no probabilities nor statistics).Basics of statistics, simple computations, use of different rules on a work project to understand the interest of this programm.\n",
      "\n",
      "Printing Summary for other.txt \n",
      "\n",
      "Total number of Sentences = 52\n",
      "Num of Sentences Reduced to 2\n",
      "Summary as follows : \n",
      "\n",
      "Honestly I think the main value added by a teacher is making the class content more approachable and understandable, hearing him present this excuse before facing many topics leaves you with the feeling that he did not really try, specially after having to check other resources to understand the topics after class and finding them much better explained in other sites.The 'R' programming language should have been taught before the class so that the students that had no background of that would be able to follow the examples easily\n",
      "\"I have learnt by myself in less time than I learnt in the class, which is a big red flag.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summarization w\n",
    "\n",
    "for dirname, _, filenames in os.walk(\"D:/OneDrive - Data ScienceTech Institute/5 python Lab\\Python Project/raw_data\"):\n",
    "    for filename in filenames:\n",
    "        print('Printing Summary for {} \\n'.format(filename))\n",
    "        summarizer = Summarizer(path = os.path.join(dirname, filename) )\n",
    "        summarizer.SummarizeMyText(fractionToReduce = 0.05)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-latvia",
   "metadata": {},
   "source": [
    "## BLEU score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "composite-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu   # Library to compare two sentences and calculate score using BLEU\n",
    "from nltk.tokenize import word_tokenize  # NLTK library to tokenize the expected and acquired results\n",
    "\n",
    "def compare_bleu(result, expected):    \n",
    "    token_res = word_tokenize(result)    # Tokenize the acquired results\n",
    "    token_exp = word_tokenize(expected)  # Tokenize the expected results\n",
    "    score = sentence_bleu(token_res, token_exp)   # Compare two sentences and calculate score\n",
    "    print(\"The BLEU score of accuracy is: \", score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "exotic-liberia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare expected summary text from Jennifer with Summary_text about student expectation \n",
    "#from automatic summary \n",
    "Summary_text = \"\"\"It was a statistics discovery, I would have appreciated to learn some basics, I had the feeling we went through tuff theroems straight away\n",
    "the class had all the probabilityI was expecting a point of view more conceptually visual (with this I mean based on real examples rather than a series of slides displaying the theoretical part black over white with coins and dices examples from time to time).The class did cover everything about probability\n",
    "Class was good but need some exercise sheet so that we can practice\"\"\"\n",
    "\n",
    "#from Jennifer \n",
    "Expected_Summary = \"\"\"In general, the professor was very knowledgeable; he \n",
    "explained all the concepts and gave lots of examples which was helpful. Some found the \n",
    "course too fast and would have liked more basic explanations. Some wanted more teaching \n",
    "on R as they didn’t know it beforehand\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mysterious-prague",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BLEU score of accuracy is:  8.972141065609098e-232\n"
     ]
    }
   ],
   "source": [
    "# BLeu score\n",
    "compare_bleu(Summary_text, Expected_Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-titanium",
   "metadata": {},
   "source": [
    "## ROUGE Score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bizarre-programming",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.3695652173913043, recall=0.19101123595505617, fmeasure=0.2518518518518518),\n",
       " 'rougeL': Score(precision=0.17391304347826086, recall=0.0898876404494382, fmeasure=0.11851851851851852)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install rouge-score\n",
    "\n",
    "#using rouge1 and rougeL\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(Summary_text, Expected_Summary)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-puppy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-collect",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-ocean",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-isaac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
